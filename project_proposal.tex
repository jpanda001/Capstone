\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
% \usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%\usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    ADDITIONAL PACKAGES    %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{enumerate}      % additional enumerate package

\title{CSC413 Project Proposal}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Jyoti Panda\\
  \And
  Walter Merjo\\
  \And
  Eron Ying
}

\begin{document}

\maketitle

\begin{abstract}
Object detection, classification and distance estimation are important perception operations of autonomous vehicles which are conducted using camera, radar and Light Detection and Ranging (LiDAR) sensors. However, data produced by LiDAR based perception systems are unreliable under certain weather and light conditions; on the other hand, radar based perception systems are impervious to those adverse conditions, but they are also more noisy, less accurate and provide sparse results. In this work, we propose a system based on radar spectra generated using multi-dimensional Fast Fourier Transform (FFT) to detect, classify and find distance to close objects in a scene. 
\end{abstract}

\section{Introduction}
With recent developments in Machine Learning (ML) and Computer Vision (CV), there have been significant advancements in the field of autonomous driving. Major automobile manufacturers have already developed Advanced Driver Assistance Systems (ADAS) that are nearing Full Self-Driving (FSD) capabilities [1]. The first major strides in the development of ADAS were made in Carnegie Mellon University through their Autonomous Land Vehicle (ALV) project in 1984 [2]. Since then, billions of dollars have been invested by corporations and academic institutions to study this area and develop reliable and practical ADAS and FSD systems [1].

Different sensors such as LiDAR, Radar, Camera, Sonar and GPS are used to perceive the environment of autonomous vehicles [3]. Visual perception operations like object detection and classification are conducted mainly using Camera, LiDAR and Radar sensors [3]. In this paper we will use 2D or 3D radar spectra, a "continuous" representation of scenes which can be generated by using a multi-dimensional FFT on raw radar Analog-to-Digital Converted (ADC) data.

\section{Related Work}
\textbf{Radar perception and data.} Image-based deep learning methods deliver strong performance for complex image understanding and manipulation tasks such as object detection and classification. Inspired by their strong performance, researchers studied the usefulness of deep learning methods in performing radar perception tasks [7]â€“[9]. However, as noted in [10], most of the aforementioned research has yielded unsatisfactory results because of the use of sparse point-cloud based representation or low-resolution spectrum-based representations. Due to recent advancements in radar technology, radars capable of generating high-resolution spectrum-based representations, such as Texas Instrument AWR22431\footnote{https://www.ti.com/tool/AWR2243BOOST}, are now easily accessible. Thus, more complex and higher capacity deep learning models can be used to yield exceptional radar perception results.

\textbf{Network architectures (YOLO and Faster R-CNN).} There are two different architectures for image-based object detection networks: One-Stage networks and Two-Stage networks. One-Stage networks predict all bounding boxes (detects objects) and assigns classes with only a single pass through a deep neural network [12]. On the other hand, Two-Stage networks (such as Faster R-CNN [x]) first generate region proposals which are about regions of an image that potentially contain objects. Then, feature maps of these regions are individually sent to a classification model to make separate class predictions for each object region proposal [11]. This method works very well but it is also very slow compared to state-of-art One-Stage networks [12], such as YOLOv5 [13][14]. Though if we adapt both architectures for usage with Range-Azimuth-Doppler (RAD) tensors, they perform with similar precision [15].

\section{Method}
Since a radar spectrum is a "continuous" representation just like an image, image-based deep learning networks can be adapted to work with it. With deep learning networks, unique patterns about the detection intensities and doppler effects of different objects can be learned, making radar object classification process much simpler. Additionally, fusion based perception frameworks that take camera images and radar spectra can also yield significantly better results compared to any traditional radar pointcloud based perception frameworks. Hence, our objective will be to create an object detection and classification pipeline that uses spectrum-based radar scene representations, and (hopefully) performs significantly better than radar point-cloud based object detection and classification pipelines. We achieve this objective through the following steps:
\begin{enumerate}[I.]
    \item Select a radar or obtain a radar dataset that can provide high-resolution raw data.
    \item Implement an efficient and low-noise DSP algorithm to generate a spectrum-based radar scene
    representation from raw radar ADC data.
    \item Implement an algorithm that can automatically annotate spectrum-based radar representations with a high degree of accuracy.
    \item Design a spectrum-based radar object detection and classification network that has significantly higher accuracy than radar point-cloud based detection and classification network, but at a comparable computational cost.
    \item (Stretch Goal) Design a fusion based object detection and classification pipeline that uses both radar spectra and camera images, and improves upon the standalone spectrum-based radar object detection and classification model.
\end{enumerate}
In steps IV and V, we'll use YOLOv5 and Faster R-CNN as our One-Stage and Two-Stage networks, respectively. After implementing the classification networks, we will compare their accuracy and speed.

\begin{thebibliography}{99}
\scriptsize
\bibitem{1}
\bibitem{2}
\bibitem{3}
\bibitem{4}
\bibitem{5}
\bibitem{6}
\bibitem{7}
\bibitem{8}
\bibitem{9}
\bibitem{10}
\bibitem{11}
\bibitem{12}
\bibitem{13}
\end{thebibliography}
\medskip


\end{document}